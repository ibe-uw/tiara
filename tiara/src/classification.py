import gzip
from typing import Dict, Union, List
import warnings
from contextlib import suppress

from Bio.SeqIO.FastaIO import SimpleFastaParser
from Bio.SeqIO.QualityIO import FastqGeneralIterator
from tqdm import tqdm
from joblib import Parallel, delayed

from skorch import NeuralNetClassifier
import torch

from tiara.src.prediction import Prediction, SingleResult
from tiara.src.models import NNet1, NNet2
from tiara.src.transformations import Transformer, TfidfWeighter
from tiara.src.utilities import parse_params, chop, time_context_manager


def fun(seq, layer, transformer, fragment_len):
    chopped = chop(seq, fragment_len)
    return transformer.transform(chopped)


allowed_letters = set("ACGT")


class Classification:
    """Class that performs classification given neural net and tf-idf models provided.

    Methods
    -------
        classify: classifies an entire fasta file
    """

    def __init__(
        self,
        min_len: int,
        nnet_weights: List[str],
        params: List[Union[Dict[str, int], str]],
        tfidf: List[str],
        threads: int = 1,
        models=(NNet1, NNet2),
    ):
        """Init method.

        Parameters
        ----------
            min_len: minimal length of the sequence to classify
            nnet_weights: a list of paths to nnet weights (generated by skorch for the nnet provided)
            params: either a list of dicts containing neural net params such as number of nodes in hidden layers
                and other parameters such as fragment lenth, kmer length etc,
                or a list of filepaths to .csv files containing parameters
                refer to utilities.parse_params docstring for a specific example
            tfidf: a list of filepaths to tf-idf model
            models: an iterable of torch model classes describing model used
        """
        self.threads = threads
        self.params = [
            parse_params(param) if isinstance(param, str) else param for param in params
        ]
        self.nnets = []
        for model, nnet_weight, params_dict in zip(models, nnet_weights, self.params):
            params_filtered = {
                key: value
                for key, value in params_dict.items()
                if key not in ["k", "fragment_len", "prob_cutoff", "fname"]
            }
            params_filtered.update({"dim_in": 4 ** params_dict["k"]})
            module = model(**params_filtered)
            net = NeuralNetClassifier(module, lr=0.0001, criterion=torch.nn.NLLLoss)
            net.initialize()
            net.load_params(f_params=nnet_weight)
            self.nnets.append(net)
        self.transformers = [
            TfidfWeighter.load_params(tfidf_param_folder)
            for tfidf_param_folder in tfidf
        ]
        self.min_len = min_len
        self.layers = len(nnet_weights)
        self.predictors = [
            Prediction(
                prob_cutoff=self.params[layer]["prob_cutoff"],
                layer=layer,
                nnet=self.nnets[layer],
                fragment_len=self.params[layer]["fragment_len"],
                k=self.params[layer]["k"],
                tnf=self.transformers[layer],
                transformer=Transformer(
                    fragment_len=self.params[layer]["fragment_len"],
                    k=self.params[layer]["k"],
                    model=self.transformers[layer],
                ),
            )
            for layer in range(self.layers)
        ]

    def classify(self, sequences_fname: str, verbose=False) -> List[SingleResult]:
        """Perform a two-step classification.

        Parameters
        ----------
            sequences_fname : a path to fasta/fastq file to classify

        Returns
        -------
            predictions: a list of lists containing SingleResult objects.
        """
        if sequences_fname.endswith(".gz"):
            with gzip.open(sequences_fname, "rt") as sequences_handle:
                seqs = list(FastqGeneralIterator(sequences_handle) 
                            if sequences_fname.lower().endswith((".fq.gz", ".fastq.gz")) 
                            else SimpleFastaParser(sequences_handle))
        else:
            with open(sequences_fname, "r") as sequences_handle:
                seqs = list(FastqGeneralIterator(sequences_handle) 
                            if sequences_fname.lower().endswith((".fq", ".fastq")) 
                            else SimpleFastaParser(sequences_handle))
        seqs = [x for x in seqs if len(x[1]) >= self.min_len]

        do = delayed(fun)
        executor = Parallel(n_jobs=self.threads)
        tasks = (
            do(x[1], 0, self.predictors[0].transformer, self.params[0]["fragment_len"])
            for x in seqs
        )
        cont_manager = (
            time_context_manager("Calculating first stage sequence representations")
            if verbose
            else suppress()
        )
        with cont_manager:
            seqs = list(
                zip([x[0] for x in seqs], [x[1] for x in seqs], executor(tasks))
            )

        # Two-step classification
        if verbose:
            print("Performing first stage of classification.")
            fst_stage_results = []
            for seq in tqdm(seqs):
                fst_stage_results.append(self.predictors[0].make_prediction(seq))
        else:
            # tasks = (do(seq) for seq in seqs)
            # fst_stage_results = executor(tasks)
            fst_stage_results = [
                self.predictors[0].make_prediction(seq) for seq in seqs
            ]
        if verbose:
            print("Done")
        predictions = []
        to_second_stage = []
        for prediction in fst_stage_results:
            if prediction.cls[0] == "organelle":
                to_second_stage.append(prediction)
            else:
                predictions.append(prediction)
        if to_second_stage:
            tasks = (
                do(
                    record.seq,
                    1,
                    self.predictors[1].transformer,
                    self.params[1]["fragment_len"],
                )
                for record in to_second_stage
            )
            cont_manager = (
                time_context_manager(
                    "Calculating second stage sequence representations"
                )
                if verbose
                else suppress()
            )
            with cont_manager:
                seqs2 = list(
                    zip(
                        [record.desc for record in to_second_stage],
                        [record.seq for record in to_second_stage],
                        executor(tasks),
                    )
                )
            if verbose:
                print("Performing second stage of classification.")
                snd_stage_results = []
                for seq in tqdm(seqs2):
                    snd_stage_results.append(self.predictors[1].make_prediction(seq))
            else:
                # tasks = (do_second_stage(seq) for seq in seqs2)
                # snd_stage_results = executor(tasks)
                snd_stage_results = [
                    self.predictors[1].make_prediction(seq) for seq in seqs2
                ]
            for fst, snd in zip(to_second_stage, snd_stage_results):
                assert fst.desc == snd.desc, "Descriptions not the same"
                assert fst.seq == snd.seq, "Sequences not the same"
                predictions.append(
                    SingleResult(
                        desc=fst.desc,
                        seq=fst.seq,
                        cls=[fst.cls[0], snd.cls[1]],
                        probs=[fst.probs[0], snd.probs[1]],
                    )
                )
        return predictions
